\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin = 0.8in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{float}
\usepackage{enumitem}

\DeclareMathOperator*{\wrote}{Wrote}
\DeclareMathOperator*{\copyof}{CopyOf}
\DeclareMathOperator*{\owns2}{Owns}
\DeclareMathOperator*{\sings}{Sings}


\title{CS534 - HW 1}
\author{Keith Chester}
\date{Due date: June 12 2022}

\begin{document}
\maketitle

\section*{Problem 1}

In problem 1, we are tasked with creating a recursive and linear time agent for following propositonal logic statements. The work associated with this problem can be found in \textit{problem1.py}.

\section*{Problem 2}

In this problem we are exploring a first-order logical knowledge base and writing out logical expressions utilizing it. The knowledge base is represented as:

\begin{itemize}
    \item $\copyof(d, a)$ - Predicate - Disk $d$ is a copy of album $a$
    \item $\owns2(p, d)$ - Predicate - Person $p$ owns disk $d$
    \item $\sings(p, s, a)$ - Predicate - Album $a$ inludes a recording of song $s$ sung by person $p$
    \item $\wrote(p, s)$ - Person $p$ wrote song $s$

\end{itemize}

We are also injecting the following constants:
\begin{itemize}
    \item $McCartney$ - a person
    \item $Gershwin$ - a person
    \item $BHoliday$ - a person
    \item $Joe$ - a person
    \item $EleanorRigby$ - a song
    \item $TheManILove$ - a song
    \item $Revolver$ - an album
\end{itemize}

\noindent Within this, we express the following statements using first-order logic:

\begin{enumerate}[label=(\alph*)]
    \item $\wrote(Gershwin, TheManILove)$ %a
    \item $\neg \wrote(Gershwin, EleanorRigby)$ %b
    \item $\wrote(Gershwin, TheManILove) \lor \wrote(McCartney, TheManILove)$ %c
    \item $\exists s \wrote(Joe, s)$ %d
    \item $\exists d \owns2(Joe, CopyOf(d, Revolver))$
    \item $\forall s \sings(McCartney, s, Revolver) \Rightarrow \wrote(McCartney, s)$ %f
    \item $\forall s \sings(p, s, Revolver) \neg \wrote(Gershwin, s)$ %g
    \item $\forall s \wrote(Gershwin, s) \Rightarrow \sings(p, s, a)$ %h
    \item $\exists a \forall s \sings(p, s, a) \wrote(Joe, s)$ %i
    \item $\exists d, a \owns2(Joe, \copyof(d, a)) \land \sings(BHoliday, s, a)$ %j
    \item $\exists d_i \copyof(d_i, a) \forall a \sings(McCartney, a, s) \land \forall  \owns2(Joe, d)$ %k
    \item $\exists d \copyof(d, a) \forall a \sings(BHoliday, s, a) \land \owns2(Joe, d)$ %l
\end{enumerate}

\section*{Problem 3}

In this queston, we are looking at the following table of three binary input atributes, and a singular binary output:

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_1$ & $1$ & $0$ & $0$ & $0$\\
        $x_2$ & $1$ & $0$ & $1$ & $0$\\
        $x_3$ & $0$ & $1$ & $0$ & $0$\\
        $x_4$ & $1$ & $1$ & $1$ & $1$\\
        $x_5$ & $1$ & $1$ & $0$ & $1$\\
    \end{tabular}
\end{center}

\subsection*{a.}

Using the Gini Index, we aim to create a decision tree for this data. For Gini Index, we are looking at the probability distribution of a particular variable being wrong when randomly chosen. From this we can begin to branch our tree into a set of decisions that lead us to a particular preditctive result.

\begin{equation}
    Gini = 1-\sum^{n}_{i=1} p_i^2
\end{equation}

\noindent ...where $p_i$ is the probability of a given object being classified into a particular class.

So, calculating the Gini index for our given data:

\noindent For $A_1$, we are look at the indexes of positive and negative outputs:

\begin{equation}
    {A_1=1,y=1} \rightarrow \frac{2}{4} = 0.5
\end{equation}
\begin{equation}
    {A_1=1,y=0} \rightarrow \frac{2}{4} = 0.5
\end{equation}
\noindent ...for the Gini calculation of:
\begin{equation}
    1-(0.5^2 + 0.5^2) = 0.5
\end{equation}
\begin{equation}
    {A_1=0,y=1} \rightarrow \frac{0}{1} = 0
\end{equation}
\begin{equation}
    {A_1=0,y=0} \rightarrow \frac{1}{1} = 1
\end{equation}
\noindent ...for the Gini calculation of:
\begin{equation}
    1-(0^2 + 1^2) = 0
\end{equation}
\noindent and the weighted sum of these Gini indexes to create the Gini index for $A_1$:
\begin{equation}
    \sum p_i * g_i = (\frac{4}{5}*0.5 + \frac{1}{5}*0) = 0.4
\end{equation}

\noindent Now we continue this for $A_2$:

\begin{equation}
    {A_2=1,y=1} \rightarrow \frac{2}{3} = 0.67
\end{equation}
\begin{equation}
    {A_2=1,y=0} \rightarrow \frac{1}{3} = 0.33
\end{equation}
\noindent ...for the Gini calculation of:
\begin{equation}
    1-(0.67^2 + 0.33^2) = 0.44
\end{equation}
\begin{equation}
    {A_2=0,y=1} \rightarrow \frac{0}{2} = 0
\end{equation}
\begin{equation}
    {A_2=0,y=0} \rightarrow \frac{2}{2} = 1
\end{equation}
\noindent ...for the Gini calculation of:
\begin{equation}
    1-(0^2 + 1^2) = 0
\end{equation}
\noindent and the weighted sum of these Gini indexes to create the Gini index for $A_2$:
\begin{equation}
    \sum p_i * g_i = (\frac{3}{5}*0.44 + \frac{2}{5}*0) = 0.27
\end{equation}

\noindent ...and finally, following the process for $A_3$:

\begin{equation}
    {A_3=1,y=1} \rightarrow \frac{1}{2} = 0.5
\end{equation}
\begin{equation}
    {A_3=1,y=0} \rightarrow \frac{1}{2} = 0.5
\end{equation}
\noindent ...for the Gini calculation of:
\begin{equation}
    1-(0.5^2 + 0.5^2) = 0.5
\end{equation}
\begin{equation}
    {A_3=0,y=1} \rightarrow \frac{1}{3} = 0.33
\end{equation}
\begin{equation}
    {A_3=0,y=0} \rightarrow \frac{2}{3} = 0.67
\end{equation}
\noindent ...for the Gini calculation of:
\begin{equation}
    1-(0.33^2 + 0.67^2) = 0.44
\end{equation}
\noindent and the weighted sum of these Gini indexes to create the Gini index for $A_3$:
\begin{equation}
    \sum p_i * g_i = (\frac{2}{5}*0.5 + \frac{3}{5}*0.44) = 0.27
\end{equation}

\noindent So the Gini indexes for each input is 

\begin{center}
    \begin{tabular}{l r}
        Input & Gini index \\
        $A_1$ & 0.4 \\
        $A_2$ & 0.27 \\
        $A_3$ & 0.27
    \end{tabular}
\end{center}

Both $A_3$ and $A_2$ have the same Gini index, thus either can be utilized as the root node of our decision tree. For our purposes, we will choose $A_3$. From this we repeat the computations above for the positive and negative branches of our $A_3$ input.

\noindent \textbf{Positive $A_3$:}

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_2$ & $1$ & $0$ & $1$ & $0$\\
        $x_4$ & $1$ & $1$ & $1$ & $1$\\
    \end{tabular}
\end{center}

\noindent Gini Index of $A_1=1$ when $A_3=1$:

\begin{equation}
    {A_1=1,y=1} \rightarrow \frac{1}{2} = 0.5
\end{equation}
\begin{equation}
    {A_1=1,y=0} \rightarrow \frac{1}{2} = 0.5
\end{equation}
\begin{equation}
    1 - (0.5^2 + 0.5^2) = 0.5
\end{equation}

\noindent Gini Index of $A_1=0$ when $A_3=1$ is not applicable, as there is no examples anymore of $A_1$ = 0. Thus our weighted Gini Index is still $0.5$.

\noindent Gini Index of $A_2=1$ when $A_3=1$:

\begin{equation}
    {A_2=1,y=1} \rightarrow \frac{1}{1} = 1.0
\end{equation}
\begin{equation}
    {A_2=1,y=0} \rightarrow \frac{0}{1} = 0.0
\end{equation}
\begin{equation}
    1 - (1.0^2 + 0.0^2) = 0.0
\end{equation}

\noindent Ginin  Index of $A_2=0$ when $A_3=1$:

\begin{equation}
    {A_2=1,y=1} \rightarrow \frac{0}{1} = 0.0
\end{equation}
\begin{equation}
    {A_2=1,y=0} \rightarrow \frac{1}{1} = 1.0
\end{equation}
\begin{equation}
    1 - (0^2 + 1.0^2) = 0.0
\end{equation}

\noindent ...for a Gini index, when weighted, to still be $0.0$ for $A_2$.

\begin{center}
    \begin{tabular}{l r}
        Input & Gini index \\
        $A_1$ & 0.5 \\
        $A_2$ & 0.0 \\
    \end{tabular}
\end{center}

\noindent ...so we see that we want to use $A_2$ for the next split as it has the lowest Gini index, and completes this branch of the tree as we have a perfect split.

\noindent \textbf{Negative $A_3$:}

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_1$ & $1$ & $0$ & $0$ & $0$\\
        $x_3$ & $0$ & $1$ & $0$ & $0$\\
        $x_5$ & $1$ & $1$ & $0$ & $1$\\
    \end{tabular}
\end{center}

\noindent Gini Index of $A_1=1$ when $A_3=0$:

\begin{equation}
    {A_1=1,y=1} \rightarrow \frac{1}{2} = 0.5
\end{equation}
\begin{equation}
    {A_1=1,y=0} \rightarrow \frac{1}{2} = 0.5
\end{equation}
\begin{equation}
    1 - (0.5^2 + 0.5^2) = 0.5
\end{equation}

\noindent Gini Index of $A_1=0$ when $A_3=0$:

\begin{equation}
    {A_1=1,y=1} \rightarrow \frac{0}{1} = 0
\end{equation}
\begin{equation}
    {A_1=1,y=0} \rightarrow \frac{1}{1} = 1.0
\end{equation}
\begin{equation}
    1 - (1.0^2 + 0.0^2) = 0.0
\end{equation}

\noindent ...for the weighted sum of a Gini index of $A_1$ when $A_3=0$:

\begin{equation}
    \sum p_i * g_i = (\frac{2}{3}*0.5 + \frac{1}{3}*0) = 0.33
\end{equation}

\noindent Moving onto the Gini Index of $A_2$ when $A_3=0$, we first explore when $A_2=1$:

\begin{equation}
    {A_2=1,y=1} \rightarrow \frac{1}{2} = 0.5
\end{equation}
\begin{equation}
    {A_2=1,y=0} \rightarrow \frac{1}{2} = 0.5
\end{equation}
\begin{equation}
    1 - (0.5^2 + 0.5^2) = 0.5
\end{equation}

\noindent Looking at $A_2=0$ when $A_3=0$:

\begin{equation}
    {A_2=1,y=1} \rightarrow \frac{0}{1} = 0.0
\end{equation}
\begin{equation}
    {A_2=1,y=0} \rightarrow \frac{1}{1} = 1.0
\end{equation}
\begin{equation}
    1 - (1.0^2 + 0.0^2) = 0.0
\end{equation}

\noindent ...and our weight index for $A_2$ when $A_3=0$ is:

\begin{equation}
    \sum p_i * g_i = (\frac{2}{3}*0.5 + \frac{1}{3}*0) = 0.33
\end{equation}

\noindent for our collected Gini indexs being:

\begin{center}
    \begin{tabular}{l r}
        Input & Gini index \\
        $A_1$ & 0.33 \\
        $A_2$ & 0.33 \\
    \end{tabular}
\end{center}

\noindent ...thus making it inconclusive which of $A_1$ or $A_2$ we could branch off of. We shall choose $A_1$ for our purposes. One more branch to go!

This next branch doesn't need much in the way of calculations; to save time, we can just observe that when $A_3=0$ and $A_1=1$, $A_2=y$. When $A_3=0$ and $A_1=0$, $y=0$.

Thus we exhaustively formed the following tree:

\begin{figure}[H]
    \centering
    \includegraphics[width = 0.65\textwidth]{problem3a.png}
\end{figure}

\subsection*{b.}

Now we utilize information gain to create a decision tree for this data. Here we will be calculating the amount of entropy for each branch. The highest information gain is the columns with the least entropy. We calculate entropy via:

\begin{equation}
    -\sum{c}{i=1}p(x_i)\log_b{p(x_i)}
\end{equation}

\noindent ...then, when we look at the potential splitting of that column, we look at the information gain from the column via looking at the change in entropy and looking at the weighted sum of post-split entropy and comparing it back to our original entropy. We are looking for larger information gain.

\begin{equation}
    \sum^{V \epsilon A} \frac{\lvert T_V \rvert}{T} * Entropy(T_V)
\end{equation}

Looking at our table once again:

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_1$ & $1$ & $0$ & $0$ & $0$\\
        $x_2$ & $1$ & $0$ & $1$ & $0$\\
        $x_3$ & $0$ & $1$ & $0$ & $0$\\
        $x_4$ & $1$ & $1$ & $1$ & $1$\\
        $x_5$ & $1$ & $1$ & $0$ & $1$\\
    \end{tabular}
\end{center}

\noindent ...we can begin calculating the information gain for each node $A_i$.

\noindent For $A_1$:
\begin{equation}
    -(\frac{4}{5}\log_2{\frac{4}{5}}) + -(\frac{1}{5}\log_2{\frac{1}{5}}) = 0.72
\end{equation}

\noindent ...and then we look at the information gain for a potential split on $A_1$:

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_1$ & $1$ & $0$ & $0$ & $0$\\
        $x_2$ & $1$ & $0$ & $1$ & $0$\\
        $x_4$ & $1$ & $1$ & $1$ & $1$\\
        $x_5$ & $1$ & $1$ & $0$ & $1$\\
    \end{tabular}
\end{center}

\begin{equation}
    -(\frac{2}{4}\log_2{\frac{2}{4}}) + -(\frac{2}{4}\log_2{\frac{2}{4}}) = 1
\end{equation}

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_3$ & $0$ & $1$ & $0$ & $0$\\
    \end{tabular}
\end{center}

\begin{equation}
    -(\frac{0}{1}\log_2{\frac{0}{1}}) + -(\frac{1}{1}\log_2{\frac{1}{1}}) = 0
\end{equation}

\begin{equation}
    \frac{4}{5}*1 + \frac{1}{5}*0 = 0.8
\end{equation}

\noindent For $A_2$:
\begin{equation}
    -(\frac{3}{5}\log_2{\frac{3}{5}}) + -(\frac{2/5}\log_2{\frac{2}{5}}) = 0.97
\end{equation}

\noindent ...and then we look for the information gain if we split on $A_2$:

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_3$ & $0$ & $1$ & $0$ & $0$\\
        $x_4$ & $1$ & $1$ & $1$ & $1$\\
        $x_5$ & $1$ & $1$ & $0$ & $1$\\
    \end{tabular}
\end{center}

\begin{equation}
    -(\frac{2}{3}\log_2{\frac{2}{3}}) + -(\frac{1}{3}\log_2{\frac{1}{3}}) = 0.92
\end{equation}

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_1$ & $1$ & $0$ & $0$ & $0$\\
        $x_2$ & $1$ & $0$ & $1$ & $0$\\
    \end{tabular}
\end{center}

\begin{equation}
    -(\frac{0}{2}\log_2{\frac{0}{2}}) + -(\frac{2}{2}\log_2{\frac{2}{2}}) = 0.0
\end{equation}

\begin{equation}
    \frac{3}{5}*0.92 + \frac{2}{5}*0 = 0.55
\end{equation}

\noindent For $A_3$:
\begin{equation}
    -(\frac{2}{5}\log_2{\frac{2}{5}}) + -(\frac{3/5}\log_2{\frac{3}{5}}) = 0.97
\end{equation}

\noindent ...and now exploring the information gain if we split on $A_3$:

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_2$ & $1$ & $0$ & $1$ & $0$\\
        $x_4$ & $1$ & $1$ & $1$ & $1$\\
    \end{tabular}
\end{center}

\begin{equation}
    -(\frac{1}{2}\log_2{\frac{1}{2}}) + -(\frac{1}{2}\log_2{\frac{1}{2}}) = 1.0
\end{equation}

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_1$ & $1$ & $0$ & $0$ & $0$\\
        $x_3$ & $0$ & $1$ & $0$ & $0$\\
        $x_5$ & $1$ & $1$ & $0$ & $1$\\
    \end{tabular}
\end{center}

\begin{equation}
    -(\frac{1}{3}\log_2{\frac{1}{3}}) + -(\frac{2}{3}\log_2{\frac{2}{3}}) = 0.92
\end{equation}

\begin{equation}
    \frac{2}{5}*1.0 + \frac{3}{5}*0.92 = 0.95
\end{equation}

\noindent ...which leads us to this entropy for each $A_i$ and the information gain if we split on each.

\begin{center}
    \begin{tabular}{l r r r}
        Input & Entropy & Split Entropy & Gain\\
        $A_1$ & 0.72 & 0.80 & -0.08\\
        $A_2$ & 0.97 & 0.55 & 0.42\\
        $A_3$ & 0.97 & 0.95 & 0.02\\
    \end{tabular}
\end{center}

From this we can see we have the highest information gain if we split on $A_2$. We continue this process for each of the resulting branches from this split.

\noindent \textbf{$A_2=1$}

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_3$ & $0$ & $1$ & $0$ & $0$\\
        $x_4$ & $1$ & $1$ & $1$ & $1$\\
        $x_5$ & $1$ & $1$ & $0$ & $1$\\
    \end{tabular}
\end{center}

\noindent \textbf{$A_1$:}

\begin{equation}
    -(\frac{2}{3}\log_2{\frac{2}{3}}) + -(\frac{1}{3}\log_2{\frac{1}{3}}) = 0.92
\end{equation}

\noindent ...and now exploring the information gain if we split on $A_1$:

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_4$ & $1$ & $1$ & $1$ & $1$\\
        $x_5$ & $1$ & $1$ & $0$ & $1$\\
    \end{tabular}
\end{center}

\begin{equation}
    -(\frac{2}{2}\log_2{\frac{2}{2}}) + -(\frac{0}{2}\log_2{\frac{0}{2}}) = 0.0
\end{equation}

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_3$ & $0$ & $1$ & $0$ & $0$\\
    \end{tabular}
\end{center}

\begin{equation}
    -(\frac{0}{1}\log_2{\frac{0}{1}}) + -(\frac{1}{1}\log_2{\frac{1}{1}}) = 0.0
\end{equation}

\noindent for an obvious weighted entropy of 0.0.

\noindent \textbf{$A_3$:}

\begin{equation}
    -(\frac{1}{3}\log_2{\frac{1}{3}}) + -(\frac{2}{3}\log_2{\frac{2}{3}}) = 0.92
\end{equation}

\noindent ...and now exploring the information gain if we split on $A_3$:

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_4$ & $1$ & $1$ & $1$ & $1$\\
    \end{tabular}
\end{center}

\begin{equation}
    -(\frac{1}{1}\log_2{\frac{1}{1}}) + -(\frac{1}{1}\log_2{\frac{0}{1}}) = 0.0
\end{equation}

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_3$ & $0$ & $1$ & $0$ & $0$\\
        $x_5$ & $1$ & $1$ & $0$ & $1$\\
    \end{tabular}
\end{center}

\begin{equation}
    -(\frac{1}{2}\log_2{\frac{1}{2}}) + -(\frac{1}{2}\log_2{\frac{1}{2}}) = 1.0
\end{equation}

\begin{equation}
    \frac{1}{3} * 0.0 + \frac{2}{3} * 1.0 = 0.67
\end{equation}

\begin{center}
    \begin{tabular}{l r r r}
        Input & Entropy & Split Entropy & Gain\\
        $A_1$ & 0.92 & 0.0 & 0.92\\
        $A_3$ & 0.92 & 0.67 & 0.25 \\
    \end{tabular}
\end{center}

Thus we would further split on $A_1$. This can be mapped directly to $A_3=y$ at this point as we've hit minimum entropy, and end this node here.

Then we can begin to explore what happens for when $A_2=0$, based on our first split.

\noindent \textbf{$A_2=0$:}

\begin{center}
    \begin{tabular}{ c r r r r }
        Example & $A_1$ & $A_2$ & $A_3$ & Output $y$\\ 
        $x_1$ & $1$ & $0$ & $0$ & $0$\\
        $x_2$ & $1$ & $0$ & $1$ & $0$\\
    \end{tabular}
\end{center}

Here we don't even have to calculate much - if $A_3=0$, then $y=0$.


\begin{figure}[H]
    \centering
    \includegraphics[width = 0.65\textwidth]{problem3b.png}
\end{figure}


\section*{Problem 4}

In this section, we consider the following data input with six inputs and a singular target output:

\begin{center}
    \begin{tabular}{c r r r r r r r r r r r r r r}
        Example & $A_1$ & $A_2$ & $A_3$ & $A_4$ & $A_5$ & $A_6$ & $A_7$ & $A_8$ & $A_9$ & $A_10$ & $A_11$ & $A_12$ & $A_13$ & $A_14$ \\
        $x_1$ & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        $x_2$ & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 1 & 0 & 1 & 1 \\
        $x_3$ & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 1 \\
        $x_4$ & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 1 & 1 & 1 & 0 & 1 \\
        $x_5$ & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 1 & 0 \\
        $x_6$ & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 1 & 0 \\
        $T$ & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\

    \end{tabular}
\end{center}

When we run our code found in \textit{problem4.py}, we find that we can train both a perceptron and decision tree on this data. If given unseen data of $A_{15}:<1, 1, 0, 0, 1, 1>$ with result $T_{15}=1$, we find that both accurately predict this result.

When we generate the decision tree, we can output its resulting branches, seen as follows:

\begin{figure}[H]
    \centering
    \includegraphics[width = 0.65\textwidth]{tree.png}
\end{figure}

Our code that utilizes the perceptron makes a perceptron makes a perceptron enacting the following linear equation:

\begin{equation}
    T = 8x_1 + 0x_2 + 4x_3 - x_4 -5 4x_5 + 3x_6 - 4
\end{equation}

\end{document}